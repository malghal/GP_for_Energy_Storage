{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833719b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gpcam import GPOptimizer\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b9f0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max y:  529.5870098158235\n",
      "x data:  (2500, 1)\n",
      "y data:  (2500, 1)\n"
     ]
    }
   ],
   "source": [
    "energy_data = np.load(\"/data/Synthetic Data Generation_1/my_synthetic_energy.npy\")\n",
    "cycle_number = np.load(\"/data/Synthetic Data Generation_1/my_synthetic_cycleNum.npy\")\n",
    "\n",
    "\n",
    "\n",
    "label_size = 30\n",
    "\n",
    "num_of_datasets = 50\n",
    "\n",
    "considered_batteries = np.array([5546, 9477, 2231, 4437, 7059, 5259, 8330, 1068, 8214, 5888, 3275, 6845, 7671, \n",
    "                         299, 5038, 3503, 8673, 2236, 3644, 4980, 993, 7545, 654, 1418, 6090, 7936, 8792, \n",
    "                         6910, 2933, 2382, 9730, 8476, 1882, 7986, 7091, 4813, 3086, 3908, 1539, 8567, 2152, \n",
    "                         5738, 8646, 9692, 2661, 6766, 7230, 512, 758, 2881])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "for i in considered_batteries: plt.scatter(cycle_number,energy_data[int(i)])\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "plt.ylabel(\"Quantity of Interest\",fontsize=label_size)\n",
    "plt.show()\n",
    "\n",
    "print(\"max y: \", np.max(energy_data))\n",
    "\n",
    "# All Data\n",
    "x_data_all = np.tile(cycle_number, (num_of_datasets, 1)).reshape(-1, 1) # repeat cycle 20 times to create x_data\n",
    "y_data_all = np.vstack(energy_data[considered_batteries, :]).reshape(-1, 1)\n",
    "x_pred = np.linspace(0,1000,1001).reshape(-1,1)\n",
    "\n",
    "print(\"x data: \", x_data_all.shape)\n",
    "print(\"y data: \", y_data_all.shape)\n",
    "           \n",
    "#plt.scatter(x_data,y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69d91f-2afc-4283-905a-440b9a3d5a7f",
   "metadata": {},
   "source": [
    "# Creating the Subfolder in Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4aeb6f-cc3b-40fd-a166-8a519c3f1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path for the new folder\n",
    "new_folder_path = f\"/results/Introduction Figures\"\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39aedc",
   "metadata": {},
   "source": [
    "# Defining the Standard GP Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03936659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "\n",
    "def my_noise_stdrd(x,hps,obj):\n",
    "    #This is a simple noise function, but can be arbitrarily complex using many hyperparameters.\n",
    "    #The noise function always has to return a matrix, because the noise can have covariances.\n",
    "    return np.diag(np.zeros((len(x))) + hps[2])\n",
    "\n",
    "def kernel_stdrd(x1,x2,hps,obj):\n",
    "    d = get_distance_matrix(x1,x2) \n",
    "\n",
    "    k = hps[0] * obj.squared_exponential_kernel(d,hps[1]) \n",
    "    return k\n",
    "\n",
    "\n",
    "def mean_stdrd(x, hps, obj):\n",
    "    return hps[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da39a6",
   "metadata": {},
   "source": [
    "# Fitting the GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c624f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done!\n"
     ]
    }
   ],
   "source": [
    "# In this synthetic data set, every battery has 50 data points, so if I want to take the data for the first 3 batteres I \n",
    "# take the first 150 data points; if I want the data for the first 5 batteries I take the first 250 data points ...\n",
    "\n",
    "\n",
    "condition = x_data_all < 600\n",
    "x_data = x_data_all[condition]\n",
    "y_data = y_data_all[condition]\n",
    "\n",
    "condition2 = x_data_all >= 600\n",
    "x_data_hidden = x_data_all[condition2]\n",
    "y_data_hidden = y_data_all[condition2]\n",
    "\n",
    "\n",
    "# Finding the mean of the initial capacity to be entered to the code\n",
    "my_ind = np.where(x_data<=10)\n",
    "initial_capacity = np.mean(y_data[my_ind[0]])\n",
    "\n",
    "init_hyperparameters = np.array([150, 250,   # Kernel\n",
    "                                  100, 250])                  # Mean.\n",
    "\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((4,2))\n",
    "# Kernel Sq Exp \n",
    "bounds[0] = np.array([100.,10000.])                         # Kernel Variance\n",
    "bounds[1] = np.array([10.,400.])                           # Kernel Lengthscale\n",
    "# Noise\n",
    "bounds[2] = np.array([1e-5,500.])                            # Noise Slope\n",
    "#Mean\n",
    "bounds[3] = np.array([1.,500.])                              # Noise Power\n",
    "\n",
    "\n",
    "trained_hps = np.array([421.74924913, 103.66812027,  83.73293929, 476.92930516])\n",
    "\n",
    "my_gpo = GPOptimizer(x_data,y_data,\n",
    "            init_hyperparameters = trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            #noise_variances=np.ones(y_data.shape) * 0.01, #providing noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_stdrd, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean_stdrd, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise_stdrd,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args=np.array([initial_capacity]),\n",
    "            )\n",
    "\n",
    "#my_gpo.train(hyperparameter_bounds=bounds, method='global')\n",
    "\n",
    "print(\"Training is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e78a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5468.723098021589\n",
      "[421.74924913 103.66812027  83.73293929 476.92930516]\n"
     ]
    }
   ],
   "source": [
    "x_pred =np.linspace(0,2000,1001).reshape(-1,1)\n",
    "\n",
    "mean = my_gpo.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "var =  my_gpo.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "print(my_gpo.log_likelihood(my_gpo.hyperparameters))\n",
    "print(my_gpo.hyperparameters)\n",
    "\n",
    "my_color = np.array([102, 178, 255])/255  \n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred,mean, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "plt.fill_between(np.squeeze(x_pred), mean - 2. * np.sqrt(var), mean + 2. * np.sqrt(var), alpha = 0.5, color = \"grey\", label = \"Posterior Covariance\")\n",
    "plt.scatter(x_data,y_data,s = 75, color='blue', label = \"Training\")\n",
    "plt.scatter(x_data_hidden,y_data_hidden,s = 75,color=[my_color], label = \"Testing\")\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.legend(fontsize=label_size,frameon=False,loc='lower left',markerscale=2)\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim([250,520])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size+4)\n",
    "plt.ylabel(\"Energy\",fontsize=label_size+4)\n",
    "plt.savefig('/results/Introduction Figures/Standard_GP.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5153e7",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Modified GP Model - Mean\n",
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dead794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "\n",
    "def my_noise_stdrd(x,hps,obj):\n",
    "    #This is a simple noise function, but can be arbitrarily complex using many hyperparameters.\n",
    "    #The noise function always has to return a matrix, because the noise can have covariances.\n",
    "    return np.diag(np.zeros((len(x))) + hps[2])\n",
    "\n",
    "\n",
    "def kernel_stdrd(x1,x2,hps,obj):\n",
    "    d = get_distance_matrix(x1,x2) \n",
    "\n",
    "    k = hps[0] * obj.squared_exponential_kernel(d,hps[1]) \n",
    "    return k\n",
    "\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    x0 = hps[3]\n",
    "    \n",
    "    m1 = hps[4]\n",
    "    m2 = hps[5]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "147d96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done!\n"
     ]
    }
   ],
   "source": [
    "# In this synthetic data set, every battery has 50 data points, so if I want to take the data for the first 3 batteres I \n",
    "# take the first 150 data points; if I want the data for the first 5 batteries I take the first 250 data points ...\n",
    "\n",
    "\n",
    "# Finding the mean of the initial capacity to be entered to the code\n",
    "my_ind = np.where(x_data <=10)\n",
    "initial_capacity = np.mean(y_data[my_ind[0]])\n",
    "\n",
    "init_hyperparameters = np.array([150, 250,                    # Kernel\n",
    "                                  50,                        # Noise\n",
    "                                  250,-0.01,-0.015])          # Mean\n",
    "\n",
    "#trained_hps = np.array([421.74924913, 103.66812027,  83.73293929, 476.92930516])\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((6,2))\n",
    "# Kernel Sq Exp \n",
    "bounds[0] = np.array([100.,10000.])                         # Kernel Variance\n",
    "bounds[1] = np.array([10.,400.])                           # Kernel Lengthscale\n",
    "# Noise\n",
    "bounds[2] = np.array([1e-5,500.])                            # Noise Slope\n",
    "#Mean\n",
    "bounds[3] = np.array([200.,700.])                          # Mean Piecewise Intersection point\n",
    "bounds[4] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds[5] = np.array([-5e-1,-1e-3])                        # Mean Slope 2\n",
    "\n",
    "trained_hps = np.array([ 1.02857201e+02,  3.98949826e+02,  8.28074295e+01,  5.23824852e+02,\n",
    " -4.60043682e-02, -3.70545114e-01])\n",
    "\n",
    "my_gpMean = GPOptimizer(x_data,y_data,\n",
    "            #init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            init_hyperparameters = trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_stdrd, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise_stdrd,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args=np.array([initial_capacity]),\n",
    "            )\n",
    "\n",
    "#my_gpMean.train(hyperparameter_bounds=bounds, method='global')\n",
    "\n",
    "print(\"Training is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11c7d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5449.694273500741\n",
      "[ 1.02857201e+02  3.98949826e+02  8.28074295e+01  5.23824852e+02\n",
      " -4.60043682e-02 -3.70545114e-01]\n"
     ]
    }
   ],
   "source": [
    "mean_Mean = my_gpMean.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "var_Mean =  my_gpMean.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "print(my_gpMean.log_likelihood(my_gpMean.hyperparameters))\n",
    "print(my_gpMean.hyperparameters)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred,mean_Mean, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "plt.fill_between(np.squeeze(x_pred), mean_Mean - 2. * np.sqrt(var_Mean), mean_Mean + 2. * np.sqrt(var_Mean), alpha = 0.5, color = \"grey\", label = \"Posterior Variance\")\n",
    "plt.scatter(x_data,y_data,color='blue')\n",
    "plt.scatter(x_data_hidden,y_data_hidden,color=[my_color])\n",
    "#plt.legend(fontsize=label_size,frameon=False,loc='lower left')\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim([250,520])\n",
    "plt.yticks([])\n",
    "#plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "plt.savefig('/results/Introduction Figures/Mean Modified GP.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775ecc4",
   "metadata": {},
   "source": [
    "## Plotting Prior Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6cbc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameter2 = my_gpMean.hyperparameters\n",
    "\n",
    "my_prior_mean = mean2(x_pred,my_hyperparameter2,my_gpMean)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred,my_prior_mean, color=\"black\", label = \" \", linewidth = 4)\n",
    "#plt.plot(x_pred,mean_Mean, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "\n",
    "plt.scatter(x_data,y_data, color=\"blue\")\n",
    "plt.scatter(x_data_hidden,y_data_hidden,color=[my_color])\n",
    "\n",
    "#plt.title(\"Trained Prior Mean\")\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.legend(fontsize=label_size,frameon=False,loc='lower left')\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim([250,520])\n",
    "plt.yticks([])\n",
    "#plt.title(\"Fitted Model\",fontsize=label_size)\n",
    "#plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "#plt.ylabel(\"Quantity of Interest\",fontsize=label_size)\n",
    "plt.savefig('/results/Introduction Figures/Modified GP Prior Mean.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aadae5",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Modified GP Model - Noise\n",
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2693c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    my_slope     = hps[2]\n",
    "    my_pow       = hps[3]\n",
    "    my_intercept = hps[4]\n",
    "\n",
    "    my_s =  my_slope * x**my_pow + my_intercept\n",
    "\n",
    "    noise = np.diag(my_s[:,0])\n",
    "    \n",
    "    return noise\n",
    "\n",
    "def kernel_stdrd(x1,x2,hps,obj):\n",
    "    d = get_distance_matrix(x1,x2) \n",
    "\n",
    "    k = hps[0] * obj.squared_exponential_kernel(d,hps[1]) \n",
    "    return k\n",
    "\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "\n",
    "    x0 = hps[5]\n",
    "    \n",
    "    m1 = hps[6]\n",
    "    m2 = hps[7]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a9532bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done!\n"
     ]
    }
   ],
   "source": [
    "# In this synthetic data set, every battery has 50 data points, so if I want to take the data for the first 3 batteres I \n",
    "# take the first 150 data points; if I want the data for the first 5 batteries I take the first 250 data points ...\n",
    "\n",
    "\n",
    "# Finding the mean of the initial capacity to be entered to the code\n",
    "my_ind = np.where(x_data<=10)\n",
    "initial_capacity = np.mean(y_data[my_ind[0]])\n",
    "\n",
    "init_hyperparameters = np.array([150, 250,                    # Kernel\n",
    "                                  0.06, 2, 2,                 # Noise\n",
    "                                  250,-0.01,-0.015])          # Mean\n",
    "\n",
    "#trained_hps = np.array([421.74924913, 103.66812027,  83.73293929, 476.92930516])\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((8,2))\n",
    "# Kernel Sq Exp \n",
    "bounds[0] = np.array([100.,10000.])                         # Kernel Variance\n",
    "bounds[1] = np.array([10.,400.])                           # Kernel Lengthscale\n",
    "# Noise\n",
    "bounds[2] = np.array([1e-5,1.])                           # Noise Slope\n",
    "bounds[3] = np.array([1.,5.])                            # Noise Power\n",
    "bounds[4] = np.array([0.,3.])                              # Noise Intercept\n",
    "#Mean\n",
    "bounds[5] = np.array([200.,700.])                          # Mean Piecewise Intersection point\n",
    "bounds[6] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds[7] = np.array([-5e-1,-1e-3])                        # Mean Slope 2\n",
    "\n",
    "trained_hps = np.array([ 1.05086385e+02,  3.98289651e+02,  2.11823751e-05,  2.56705587e+00,\n",
    "  2.90357790e+00,  5.24274178e+02, -5.65283589e-02, -3.53173534e-01])\n",
    "\n",
    "my_gpNoise = GPOptimizer(x_data,y_data,\n",
    "            #init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            init_hyperparameters = trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_stdrd, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args=np.array([initial_capacity]),\n",
    "            )\n",
    "\n",
    "#my_gpNoise.train(hyperparameter_bounds=bounds, method='global')\n",
    "\n",
    "print(\"Training is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e27f4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4835.075850887582\n",
      "[ 1.05086385e+02  3.98289651e+02  2.11823751e-05  2.56705587e+00\n",
      "  2.90357790e+00  5.24274178e+02 -5.65283589e-02 -3.53173534e-01]\n"
     ]
    }
   ],
   "source": [
    "mean_Noise = my_gpNoise.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "var_Noise =  my_gpNoise.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "print(my_gpNoise.log_likelihood(my_gpNoise.hyperparameters))\n",
    "print(my_gpNoise.hyperparameters)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred,mean_Noise, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "plt.fill_between(np.squeeze(x_pred), mean_Noise - 2. * np.sqrt(var_Noise), mean_Noise + 2. * np.sqrt(var_Noise), alpha = 0.5, color = \"grey\", label = \"Posterior Variance\")\n",
    "plt.scatter(x_data,y_data,color=\"blue\")\n",
    "plt.scatter(x_data_hidden,y_data_hidden,color=[my_color])\n",
    "#plt.legend(fontsize=label_size,frameon=False,loc='lower left')\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim([250,520])\n",
    "plt.yticks([])\n",
    "#plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "#plt.ylabel(\"Quantity of Interest\",fontsize=label_size)\n",
    "plt.savefig('/results/Introduction Figures/Mean_Noise Modified GP.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693191b",
   "metadata": {},
   "source": [
    "## Plotting Noise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ec3b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the variability trend in the data\n",
    "\n",
    "variances = np.var(energy_data[considered_batteries], axis=0)\n",
    "stds = np.sqrt(variances)\n",
    "\n",
    "GT_hps = np.array([0,0,                      # kernel - Doesn't Matter\n",
    "                   1.002e-05,  2.69, 3,      # Noise\n",
    "                   500,-0.05,-0.2])          # Mean\n",
    "\n",
    "GT_stds =  np.sqrt(np.diag(my_noise(cycle_number, GT_hps, my_gpNoise)))\n",
    "\n",
    "predicted_variability = np.diag(my_noise(cycle_number, my_gpNoise.hyperparameters, my_gpNoise))\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize = (10,10))\n",
    "#plt.plot(cycle_number,stds, color = \"blue\", linewidth = 3, label = \" \")\n",
    "plt.plot(cycle_number,GT_stds, color = \"blue\", linewidth = 3,linestyle = '--', label = \"Data\")\n",
    "\n",
    "plt.plot(cycle_number,np.sqrt(predicted_variability), color=\"black\", linewidth = 3, label = \"Noise Model\")\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "plt.ylabel(\"Energy Variability\",fontsize=label_size)\n",
    "plt.xlim(0,600)\n",
    "#plt.xticks([])\n",
    "plt.ylim([0,20])\n",
    "plt.yticks([0,5,10,15,20])\n",
    "\n",
    "#plt.yticks([])\n",
    "plt.xticks([0,150,300,450,600])\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.legend(fontsize=label_size,frameon=False,loc='upper left')\n",
    "#plt.savefig('Modified GP Noise Model.pdf', bbox_inches='tight')\n",
    "plt.savefig('/results/Introduction Figures/Fitted_noise_model.png', dpi=300)  # Specify the desired resolution (e.g., 300 dpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dadff",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Modified GP Model - All\n",
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac42221",
   "metadata": {},
   "source": [
    "# Defining the GP Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ef33a",
   "metadata": {},
   "source": [
    "## Deep Kernel Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b00ceeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All remaining code asssumes that the NN architecture is made up of two hidden layers and same number of nodes\n",
    "# If other architectures are used, the indices of the hyperparameters and their boudsn need to be changed accordingly\n",
    "# Number of nodes can be varied as the user prefer\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nodes_num = 5\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.layer1 = nn.Linear(1, self.nodes_num)\n",
    "        self.layer2 = nn.Linear(self.nodes_num, self.nodes_num)\n",
    "        self.layer3 = nn.Linear(self.nodes_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        #print(x)\n",
    "        x = torch.Tensor(x)\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        x = torch.nn.functional.relu(self.layer3(x))\n",
    "        return x.detach().numpy()\n",
    "\n",
    "    def set_weights(self,w1,w2,w3):\n",
    "        with torch.no_grad():\n",
    "            self.layer1.weight = nn.Parameter(torch.from_numpy(w1).float())\n",
    "            self.layer2.weight = nn.Parameter(torch.from_numpy(w2).float())\n",
    "            self.layer3.weight = nn.Parameter(torch.from_numpy(w3).float())\n",
    "\n",
    "    def set_biases(self,b1,b2,b3):\n",
    "        with torch.no_grad():\n",
    "            self.layer1.bias = nn.Parameter(torch.from_numpy(b1).float())\n",
    "            self.layer2.bias = nn.Parameter(torch.from_numpy(b2).float())\n",
    "            self.layer3.bias = nn.Parameter(torch.from_numpy(b3).float())\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.layer1.weight, self.layer2.weight, self.layer3.weight\n",
    "    def get_biases(self):\n",
    "        return self.layer1.bias, self.layer2.bias, self.layer3.bias\n",
    "\n",
    "n = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a02ae",
   "metadata": {},
   "source": [
    "## Remaining GP Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83c7eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the squared Exponential Function\n",
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    my_slope     = hps[total_num_of_NN_hps+1]\n",
    "    my_pow       = hps[total_num_of_NN_hps+2]\n",
    "    my_intercept = hps[total_num_of_NN_hps+3]\n",
    "\n",
    "    my_s =  my_slope * x**my_pow + my_intercept\n",
    "\n",
    "    noise = np.diag(my_s[:,0])\n",
    "    \n",
    "    return noise\n",
    "\n",
    "# Kernel Function\n",
    "def kernel_nn(x1,x2,hps,obj):\n",
    "\n",
    "    nodes_num           = obj.args[0]\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    # NN\n",
    "    n.set_weights(hps[0:nodes_num].reshape(nodes_num,1),\n",
    "                  hps[nodes_num:nodes_num**2+nodes_num].reshape(nodes_num,nodes_num),\n",
    "                  hps[nodes_num**2+nodes_num:nodes_num**2+2*nodes_num].reshape(1,nodes_num))\n",
    "\n",
    "    n.set_biases(hps[nodes_num**2+2*nodes_num:nodes_num**2+3*nodes_num].reshape(nodes_num),\n",
    "                 hps[nodes_num**2+3*nodes_num:nodes_num**2+4*nodes_num].reshape(nodes_num),\n",
    "                 np.array([hps[nodes_num**2+4*nodes_num]]))\n",
    "\n",
    "    x1_nn = n.forward(x1).reshape(-1,1)\n",
    "    x2_nn = n.forward(x2).reshape(-1,1)\n",
    "    d = get_distance_matrix(x1_nn,x2_nn)\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    k = hps[total_num_of_NN_hps] * obj.squared_exponential_kernel(d, 200) #100\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    x0 = hps[total_num_of_NN_hps+4]\n",
    "    \n",
    "    m1 = hps[total_num_of_NN_hps+5]\n",
    "    m2 = hps[total_num_of_NN_hps+6]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f7d59",
   "metadata": {},
   "source": [
    "# Training the Model with Few Data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21746447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data:  (300, 1)\n",
      "y data:  (300, 1)\n",
      "Training is Done!\n"
     ]
    }
   ],
   "source": [
    "nodes_num = n.nodes_num\n",
    "\n",
    "total_num_of_NN_hps = nodes_num**2 + 4*nodes_num + 1     # Depends on the number of layers used\n",
    "num_of_other_hps = 7                                     # Depends on Kernel, noise and mean functions\n",
    "\n",
    "NN_weights_initial_hps = np.random.uniform(-1, 1,nodes_num**2+2*nodes_num)\n",
    "NN_biases_initial_hps = np.random.uniform(-250, 250,total_num_of_NN_hps-(nodes_num**2+2*nodes_num))\n",
    "\n",
    "\n",
    "other_init_hps = np.array([200,                   # Kernel\n",
    "                           0.06, 2, 2,            # Noise  \n",
    "                           250,-0.01,-0.015])     # Mean.\n",
    "\n",
    "init_hyperparameters = np.concatenate([NN_weights_initial_hps,NN_biases_initial_hps,other_init_hps])\n",
    "\n",
    "# \n",
    "x_data_trial = np.array([x_data[0:300]]).reshape(-1,1)\n",
    "y_data_trial = np.array([y_data[0:300]]).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"x data: \", x_data_trial.shape)\n",
    "print(\"y data: \", y_data_trial.shape)\n",
    "\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((total_num_of_NN_hps + num_of_other_hps,2))\n",
    "\n",
    "# NN\n",
    "bounds[0:nodes_num**2+2*nodes_num] = np.array([-1.,1.])                      # Weights NN: Define spread and shift in output\n",
    "bounds[nodes_num**2+2*nodes_num:total_num_of_NN_hps] = np.array([-250.,250.])    # Biases of NN: Define shift in output\n",
    "\n",
    "# Kernel Sq Exp \n",
    "bounds[total_num_of_NN_hps] = np.array([100.,10000.])                             # Kernel Variance\n",
    "#bounds[total_num_of_NN_hps+7] = np.array([10.,300.])                           # Kernel Lengthscale\n",
    "\n",
    "# Noise\n",
    "bounds[total_num_of_NN_hps+1] = np.array([1e-5,1.])                           # Noise Slope\n",
    "bounds[total_num_of_NN_hps+2] = np.array([1.,5.])                            # Noise Power\n",
    "bounds[total_num_of_NN_hps+3] = np.array([0.,3.])                              # Noise Intercept\n",
    "# Mean\n",
    "bounds[total_num_of_NN_hps+4] = np.array([200.,700.])                          # Mean Piecewise Intersection point\n",
    "bounds[total_num_of_NN_hps+5] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds[total_num_of_NN_hps+6] = np.array([-5e-1,-1e-3])                        # Mean Slope 2\n",
    "\n",
    "\n",
    "## The Following are the trained hps when using the first 300 data points. \n",
    "## Do not train again!\n",
    "trained_hps = np.array([-0.349,  0.461, -0.678,  0.604,  0.747,  0.827,  0.282,  0.126,  0.572, -0.313,\n",
    "                   -0.463, -0.174, -0.619,  0.701, -0.109,  0.048, -0.318, -0.367, -0.499, -0.321,\n",
    "                    0.083,  0.668, -0.209,  0.552, -0.355,  0.192,  0.772, -0.739,  0.959, -0.772,\n",
    "                   -0.713, -0.817, -0.014,  0.077,  0.381, -196.313,  126.321,   45.903, -216.376,  \n",
    "                    126.7, -86.184, -60.536, -108.548, 67.332,  162.204, -36.093, 102.949, 0.0005, 2.654, 2.373,\n",
    "                    5.44103e+02, -5.30000e-02, -4.42000e-01])\n",
    "\n",
    "\n",
    "\n",
    "my_gpNN = GPOptimizer(x_data_trial,y_data_trial,\n",
    "            #init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            init_hyperparameters = trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "             #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_nn, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args= np.array([nodes_num,total_num_of_NN_hps]),\n",
    "            )\n",
    "\n",
    "#my_gpNN.train(hyperparameter_boundsbounds, method='global')\n",
    "\n",
    "print(\"Training is Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d712a531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(56.722222222222214, 0.5, 'Energy')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanNN = my_gpNN.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "varNN =  my_gpNN.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "\n",
    "plt.plot(x_pred,meanNN, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "plt.fill_between(np.squeeze(x_pred), meanNN - 2. * np.sqrt(varNN), meanNN + 2. * np.sqrt(varNN), alpha = 0.5, color = \"grey\", label = \"Posterior Variance\")\n",
    "plt.scatter(x_data_trial,y_data_trial)\n",
    "plt.legend()\n",
    "plt.xlim([0,np.max(x_pred)])\n",
    "plt.title(\"Fitted Model\")\n",
    "plt.xlabel(\"Cycle Number\")\n",
    "plt.ylabel(\"Energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "330de368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Weights: \n",
      "[-0.349  0.461 -0.678  0.604  0.747  0.827  0.282  0.126  0.572 -0.313\n",
      " -0.463 -0.174 -0.619  0.701 -0.109  0.048 -0.318 -0.367 -0.499 -0.321\n",
      "  0.083  0.668 -0.209  0.552 -0.355  0.192  0.772 -0.739  0.959 -0.772\n",
      " -0.713 -0.817 -0.014  0.077  0.381]\n",
      " \n",
      "NN Biases: \n",
      "[-196.313  126.321   45.903 -216.376  126.7    -86.184  -60.536 -108.548\n",
      "   67.332  162.204  -36.093]\n",
      " \n",
      "Sq. Expo Kernel: \n",
      "[102.949]\n",
      " \n",
      "Noise: \n",
      "[0.    2.654 2.373]\n",
      " \n",
      "Mean: \n",
      "[ 5.44103e+02 -5.30000e-02 -4.42000e-01]\n"
     ]
    }
   ],
   "source": [
    "my_hyperparameter = my_gpNN.hyperparameters\n",
    "\n",
    "print(\"NN Weights: \")\n",
    "print(np.round(my_hyperparameter[0:nodes_num**2+nodes_num*2],3))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"NN Biases: \")\n",
    "print(np.round(my_hyperparameter[nodes_num**2+nodes_num*2:total_num_of_NN_hps],3))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Sq. Expo Kernel: \")\n",
    "print(np.round(my_hyperparameter[total_num_of_NN_hps:total_num_of_NN_hps+1],3))\n",
    "\n",
    "#print(\"Sq. Expo Kernel lengthscale: \")\n",
    "#print(np.round(my_hyperparameter[total_num_of_NN_hps+7],3))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Noise: \")\n",
    "print(np.round(my_hyperparameter[total_num_of_NN_hps+1:total_num_of_NN_hps+4],3))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Mean: \")\n",
    "print(np.round(my_hyperparameter[total_num_of_NN_hps+4:total_num_of_NN_hps+7],3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e8b80",
   "metadata": {},
   "source": [
    "# Training the GP Model for all data, with the prev. identified hps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a63ed5",
   "metadata": {},
   "source": [
    "# GP Components - Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcc66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the squared Exponential Function\n",
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    my_slope     = hps[total_num_of_NN_hps+1]\n",
    "    my_pow       = hps[total_num_of_NN_hps+2]\n",
    "    my_intercept = hps[total_num_of_NN_hps+3]\n",
    "\n",
    "    my_s =  my_slope * x**my_pow + my_intercept\n",
    "\n",
    "    noise = np.diag(my_s[:,0])\n",
    "    \n",
    "    return noise\n",
    "\n",
    "# Kernel Function\n",
    "def kernel_nn(x1,x2,hps,obj):\n",
    "\n",
    "    nodes_num           = obj.args[0]\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    trained_NN_hps = np.array([-0.349,  0.461, -0.678,  0.604,  0.747,  0.827,  0.282,  0.126,  0.572, -0.313,\n",
    "                   -0.463, -0.174, -0.619,  0.701, -0.109,  0.048, -0.318, -0.367, -0.499, -0.321,\n",
    "                    0.083,  0.668, -0.209,  0.552, -0.355,  0.192,  0.772, -0.739,  0.959, -0.772,\n",
    "                   -0.713, -0.817, -0.014,  0.077,  0.381, -196.313,  126.321,   45.903, -216.376,  \n",
    "                    126.7, -86.184, -60.536, -108.548, 67.332,  162.204,  -36.093])\n",
    "    \n",
    "    # NN\n",
    "    n.set_weights(trained_NN_hps[0:nodes_num].reshape(nodes_num,1),\n",
    "                  trained_NN_hps[nodes_num:nodes_num**2+nodes_num].reshape(nodes_num,nodes_num),\n",
    "                  trained_NN_hps[nodes_num**2+nodes_num:nodes_num**2+2*nodes_num].reshape(1,nodes_num))\n",
    "\n",
    "    n.set_biases(trained_NN_hps[nodes_num**2+2*nodes_num:nodes_num**2+3*nodes_num].reshape(nodes_num),\n",
    "                 trained_NN_hps[nodes_num**2+3*nodes_num:nodes_num**2+4*nodes_num].reshape(nodes_num),\n",
    "                 np.array([trained_NN_hps[nodes_num**2+4*nodes_num]]))\n",
    "\n",
    "    x1_nn = n.forward(x1).reshape(-1,1)\n",
    "    x2_nn = n.forward(x2).reshape(-1,1)\n",
    "    d = get_distance_matrix(x1_nn,x2_nn)\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    k = hps[total_num_of_NN_hps] * obj.squared_exponential_kernel(d, 200) #100\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    x0 = hps[total_num_of_NN_hps+4]\n",
    "    \n",
    "    m1 = hps[total_num_of_NN_hps+5]\n",
    "    m2 = hps[total_num_of_NN_hps+6]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4012133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data:  (1500,)\n",
      "y data:  (1500,)\n",
      "Training is Done!\n"
     ]
    }
   ],
   "source": [
    "nodes_num = n.nodes_num\n",
    "\n",
    "total_num_of_NN_hps = 0 # nodes_num**2 + 4*nodes_num + 1     # Depends on the number of layers used\n",
    "num_of_other_hps = 7                                     # Depends on Kernel, noise and mean functions\n",
    "\n",
    "#NN_weights_initial_hps = np.random.uniform(-1, 1,nodes_num**2+2*nodes_num)\n",
    "#NN_biases_initial_hps = np.random.uniform(-250, 250,total_num_of_NN_hps-(nodes_num**2+2*nodes_num))\n",
    "\n",
    "\n",
    "init_hyperparameters = np.array([200,                   # Kernel\n",
    "                           0.06, 2, 2,            # Noise  \n",
    "                           250,-0.01,-0.015])     # Mean.\n",
    "\n",
    "print(\"x data: \", x_data.shape)\n",
    "print(\"y data: \", y_data.shape)\n",
    "\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds1 = np.empty((total_num_of_NN_hps + num_of_other_hps,2))\n",
    "\n",
    "# NN\n",
    "#bounds[0:nodes_num**2+2*nodes_num] = np.array([-1.,1.])                      # Weights NN: Define spread and shift in output\n",
    "#bounds[nodes_num**2+2*nodes_num:total_num_of_NN_hps] = np.array([-250.,250.])    # Biases of NN: Define shift in output\n",
    "\n",
    "# Kernel Sq Exp \n",
    "bounds1[total_num_of_NN_hps] = np.array([100.,10000.])                             # Kernel Variance\n",
    "#bounds[total_num_of_NN_hps+7] = np.array([10.,300.])                           # Kernel Lengthscale\n",
    "\n",
    "# Noise\n",
    "bounds1[total_num_of_NN_hps+1] = np.array([1e-5,1.])                           # Noise Slope\n",
    "bounds1[total_num_of_NN_hps+2] = np.array([1.,5.])                            # Noise Power\n",
    "bounds1[total_num_of_NN_hps+3] = np.array([0.,3.])                              # Noise Intercept\n",
    "# Mean\n",
    "bounds1[total_num_of_NN_hps+4] = np.array([200.,700.])                          # Mean Piecewise Intersection point\n",
    "bounds1[total_num_of_NN_hps+5] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds1[total_num_of_NN_hps+6] = np.array([-5e-1,-1e-3])                        # Mean Slope 2\n",
    "\n",
    "\n",
    "trained_hps = np.array([1.03696800e+02,  1.30611831e-05,  2.64659032e+00,  2.97615034e+00,\n",
    "  5.24895797e+02, -5.01939901e-02, -3.56348077e-01])\n",
    "\n",
    "\n",
    "my_gpNN1 = GPOptimizer(x_data,y_data,\n",
    "            #init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            init_hyperparameters = trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_nn, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args= np.array([nodes_num,total_num_of_NN_hps]),\n",
    "            )\n",
    "\n",
    "#my_gpNN1.train(bounds1, method='global')\n",
    "\n",
    "print(\"Training is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31309e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4829.516560099571\n",
      "[ 1.03696800e+02  1.30611831e-05  2.64659032e+00  2.97615034e+00\n",
      "  5.24895797e+02 -5.01939901e-02 -3.56348077e-01]\n"
     ]
    }
   ],
   "source": [
    "meanNN = my_gpNN1.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "varNN =  my_gpNN1.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "print(my_gpNN1.log_likelihood(my_gpNN1.hyperparameters))\n",
    "print(my_gpNN1.hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f40fec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred,meanNN, color=\"red\", label = \"Posterior Mean\", linewidth = 4)\n",
    "plt.fill_between(np.squeeze(x_pred), meanNN - 2. * np.sqrt(varNN), meanNN + 2. * np.sqrt(varNN), alpha = 0.5, color = \"grey\", label = \"Posterior Variance\")\n",
    "plt.scatter(x_data,y_data,s = 75, color='blue', label = \"Training\")\n",
    "plt.scatter(x_data_hidden,y_data_hidden,s = 75,color=[my_color], label = \"Testing\")\n",
    "\n",
    "plt.legend(fontsize=label_size,frameon=False,loc='lower left',markerscale=2)\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim([250,520])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size+4)\n",
    "plt.ylabel(\"Energy\",fontsize=label_size+4)\n",
    "#plt.savefig('Modified GP All.pdf', bbox_inches='tight')\n",
    "plt.savefig('/results/Introduction Figures/Modified GP All.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e995d",
   "metadata": {},
   "source": [
    "# Plotting the Warping of Input Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6bb320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_nn = n.forward(x_pred)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(x_pred,x_pred_nn, color=\"black\", linewidth = 0.5)\n",
    "#plt.xlabel(\"Original Space $\\mathcal{X}$\",fontsize=label_size)\n",
    "#plt.ylabel(\"Transformed Space $\\mathcal{X}^*$\",fontsize=label_size)\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlim(0,1000)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim(0,100)\n",
    "plt.yticks([])\n",
    "plt.savefig('/results/Introduction Figures/Modified GP Space Warping.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e934c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572f612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
