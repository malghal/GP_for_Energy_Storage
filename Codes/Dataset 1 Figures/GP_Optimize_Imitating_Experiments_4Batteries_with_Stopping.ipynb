{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBJqH20V-aIL"
   },
   "source": [
    "# Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50644,
     "status": "ok",
     "timestamp": 1684943496650,
     "user": {
      "displayName": "Maher Alghalayini",
      "userId": "14727376703760943485"
     },
     "user_tz": 420
    },
    "id": "Pk4bwQjL6FTh",
    "outputId": "4d0a36f2-f8fa-4cab-ee04-e176b45fec06"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from gpcam import GPOptimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3HP87fb7Bmq"
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "executionInfo": {
     "elapsed": 2642,
     "status": "ok",
     "timestamp": 1684943546061,
     "user": {
      "displayName": "Maher Alghalayini",
      "userId": "14727376703760943485"
     },
     "user_tz": 420
    },
    "id": "XQ3xNtnJ6d19",
    "outputId": "710f3683-3267-4909-ccd5-094a7694bf8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max y:  363.5404228825251\n"
     ]
    }
   ],
   "source": [
    "energy_data_total = np.load(\"/data/Dataset 1/energy.npy\")\n",
    "cycle_number_total = np.load(\"/data/Dataset 1/cycle.npy\")\n",
    "\n",
    "\n",
    "# use slicing to select every other column and all rows\n",
    "energy_data = energy_data_total[:, ::2]\n",
    "cycle_number = cycle_number_total[::2]\n",
    "\n",
    "num_of_datasets = len(energy_data[:,0])\n",
    "\n",
    "label_size = 30\n",
    "plt.figure(figsize = (20,10))\n",
    "for i in range(len(energy_data)): plt.scatter(cycle_number,energy_data[i]) \n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size) \n",
    "plt.ylabel(\"Energy\",fontsize=label_size)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Creating the cycle_data array with the same shape of energy_data, where all rows have the same values as cycle_number_range. This results in 1 row repeated 22 times.\n",
    "#cycle_data = np.full_like(energy_data, cycle_number_range)\n",
    "\n",
    "print(\"max y: \", np.max(energy_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Subfolder in Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m new_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/Dataset 1 Figures/With Stopping/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the folder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(new_folder_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path for the new folder\n",
    "new_folder_path = f\"/results/Dataset 1 Figures/With Stopping/\"\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Kernel Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All remaining code asssumes that the NN architecture is made up of two hidden layers and same number of nodes\n",
    "# If other architectures are used, the indices of the hyperparameters and their boudsn need to be changed accordingly\n",
    "# Number of nodes can be varied as the user prefer\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nodes_num = 5\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.layer1 = nn.Linear(1, self.nodes_num)\n",
    "        self.layer2 = nn.Linear(self.nodes_num, self.nodes_num)\n",
    "        self.layer3 = nn.Linear(self.nodes_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        #print(x)\n",
    "        x = torch.Tensor(x)\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        x = torch.nn.functional.relu(self.layer3(x))\n",
    "        return x.detach().numpy()\n",
    "\n",
    "    def set_weights(self,w1,w2,w3):\n",
    "      with torch.no_grad():\n",
    "        self.layer1.weight = nn.Parameter(torch.from_numpy(w1).float())\n",
    "        self.layer2.weight = nn.Parameter(torch.from_numpy(w2).float())\n",
    "        self.layer3.weight = nn.Parameter(torch.from_numpy(w3).float())\n",
    "\n",
    "    def set_biases(self,b1,b2,b3):\n",
    "      with torch.no_grad():\n",
    "        self.layer1.bias = nn.Parameter(torch.from_numpy(b1).float())\n",
    "        self.layer2.bias = nn.Parameter(torch.from_numpy(b2).float())\n",
    "        self.layer3.bias = nn.Parameter(torch.from_numpy(b3).float())\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.layer1.weight, self.layer2.weight, self.layer3.weight\n",
    "    def get_biases(self):\n",
    "        return self.layer1.bias, self.layer2.bias, self.layer3.bias\n",
    "\n",
    "n = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GokqOTH77JkS"
   },
   "source": [
    "# Defining the GP Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1684943550025,
     "user": {
      "displayName": "Maher Alghalayini",
      "userId": "14727376703760943485"
     },
     "user_tz": 420
    },
    "id": "TOFcQoaLD5S1"
   },
   "outputs": [],
   "source": [
    "# For the squared Exponential Function\n",
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    my_slope     = hps[total_num_of_NN_hps+1]\n",
    "    my_pow       = hps[total_num_of_NN_hps+2]\n",
    "    my_intercept = hps[total_num_of_NN_hps+3]\n",
    "\n",
    "    my_s = my_slope * x**my_pow + my_intercept\n",
    "    noise = np.diag(my_s[:,0])\n",
    "    \n",
    "    return noise\n",
    "\n",
    "# Kernel Function\n",
    "def kernel_nn(x1,x2,hps,obj):\n",
    "\n",
    "    nodes_num           = obj.args[0]\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    trained_NN_hps      = obj.args[3:][0]\n",
    "    \n",
    "    # NN\n",
    "    n.set_weights(trained_NN_hps[0:nodes_num].reshape(nodes_num,1),\n",
    "                  trained_NN_hps[nodes_num:nodes_num**2+nodes_num].reshape(nodes_num,nodes_num),\n",
    "                  trained_NN_hps[nodes_num**2+nodes_num:nodes_num**2+2*nodes_num].reshape(1,nodes_num))\n",
    "\n",
    "    n.set_biases(trained_NN_hps[nodes_num**2+2*nodes_num:nodes_num**2+3*nodes_num].reshape(nodes_num),\n",
    "                 trained_NN_hps[nodes_num**2+3*nodes_num:nodes_num**2+4*nodes_num].reshape(nodes_num),\n",
    "                 np.array([trained_NN_hps[nodes_num**2+4*nodes_num]]))\n",
    "\n",
    "    x1_nn = n.forward(x1).reshape(-1,1)\n",
    "    x2_nn = n.forward(x2).reshape(-1,1)\n",
    "    d = get_distance_matrix(x1_nn,x2_nn)\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    k = hps[total_num_of_NN_hps] * obj.squared_exponential_kernel(d, 50) #100\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    x0 = hps[total_num_of_NN_hps+4]\n",
    "    \n",
    "    m1 = hps[total_num_of_NN_hps+5]\n",
    "    m2 = hps[total_num_of_NN_hps+6]\n",
    "\n",
    "    b1  = obj.args[2]\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distribution(m,v,x_pred):\n",
    "    distr = np.zeros((len(x_pred),1000))\n",
    "    y = np.linspace(0,360,1000)\n",
    "    std = np.sqrt(v)\n",
    "    for i in range(len(x_pred)):\n",
    "        distr[i] = 1./(std[i] * np.sqrt(2 * np.pi)) * np.exp( - (y - m[i])**2 / (2. * std[i]**2))\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Ground Truth Probability of Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 20\n",
    "\n",
    "x_data_all = np.tile(cycle_number, data_size).reshape(-1, 1)   # repeat cycle 20 times to create x_data\n",
    "y_data_all = np.vstack(energy_data[0:20,:]).reshape(-1, 1)                  # stack energy rows to create y_data\n",
    "\n",
    "\n",
    "nodes_num = n.nodes_num\n",
    "\n",
    "total_num_of_NN_hps = 0 # nodes_num**2 + 4*nodes_num + 1     # Depends on the number of layers used\n",
    "num_of_other_hps = 7                                     # Depends on Kernel, noise and mean functions\n",
    "\n",
    "\n",
    "init_hyperparameters = np.array([1500,              # Kernel\n",
    "                           0.06, 2, 2,             # Noise  \n",
    "                           250,-0.01,-0.015])      # Mean.\n",
    "\n",
    "\n",
    "# Trained NN hyperparameters\n",
    "#     These hyperparameters were trained a priori to lower the computational requirements during the battery testing sequence\n",
    "#     The user has the choice to train these NN hyperparameters with those of the Kernel, Noise, and mean functions\n",
    "NN_weights_trained_hps = np.array([0.157, 0.149, 0.029, 0.061, 0.016, 0.219, 0.436, 0.148, 0.461, 1.772, 1.945, 0.084,\n",
    "                                   1.743, 0.731, 1.881, 1.342, 1.015, 0.982, 1.795, 0.4,   1.062, 0.753, 1.052, 1.252,\n",
    "                                   1.557, 0.708, 0.149, 1.348, 1.481, 0.568, 0.028, 0.036, 0.107, 0.05,  0.101])\n",
    "\n",
    "NN_biases_trained_hps = np.array([1.168, 1.915, 0.184, 0.145, 0.893, 0.377, 1.289, 0.911, 1.971, 0.339, 1.573])\n",
    "\n",
    "my_trained_NN_hps = np.concatenate([NN_weights_trained_hps,NN_biases_trained_hps])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Training Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/2048398358.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    }
   ],
   "source": [
    "dx = 1000./1000.\n",
    "  \n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((total_num_of_NN_hps + num_of_other_hps,2))\n",
    "\n",
    "# Kernel Sq Exp \n",
    "bounds[total_num_of_NN_hps] = np.array([500.,7000.])                             # Kernel Variance\n",
    "#bounds[total_num_of_NN_hps+7] = np.array([10.,300.])                           # Kernel Lengthscale\n",
    "\n",
    "# Noise\n",
    "bounds[total_num_of_NN_hps+1] = np.array([1e-5,15.])                           # Noise Slope\n",
    "bounds[total_num_of_NN_hps+2] = np.array([1,5.])                            # Noise Power\n",
    "bounds[total_num_of_NN_hps+3] = np.array([0.,5.])                              # Noise Intercept\n",
    "# Mean\n",
    "bounds[total_num_of_NN_hps+4] = np.array([200.,450.])                          # Mean Piecewise Intersection point\n",
    "bounds[total_num_of_NN_hps+5] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds[total_num_of_NN_hps+6] = np.array([-1e-1,-1e-2])                        # Mean Slope 2\n",
    "\n",
    "\n",
    "trained_hps = np.array([ 3.69988606e+03,  1.00111228e-05,  2.96700098e+00,  2.25831837e+00,\n",
    "  2.04048171e+02, -2.78484076e-02, -9.82988265e-02])\n",
    "\n",
    "\n",
    "    \n",
    "# Finding the mean of the initial capacity to be entered to the code\n",
    "my_ind = np.where(x_data_all<=10)\n",
    "initial_capacity = np.mean(y_data_all[my_ind[0]])\n",
    "    \n",
    "my_gpGT = GPOptimizer(x_data_all,y_data_all,\n",
    "            #init_hyperparameters = init_hyperparameters,  \n",
    "            init_hyperparameters = trained_hps,  \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_nn, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n",
    "\n",
    " \n",
    "#my_gpGT.train(hyperparameter_bounds=bounds, method='global')\n",
    "\n",
    "my_GT_hps = my_gpGT.hyperparameters\n",
    "\n",
    "print(\"GT Training Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Hyperparameters:  [ 3.69988606e+03  1.00111228e-05  2.96700098e+00  2.25831837e+00\n",
      "  2.04048171e+02 -2.78484076e-02 -9.82988265e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"GT Hyperparameters: \", my_gpGT.hyperparameters)\n",
    "\n",
    "x_pred = np.linspace(0,600,1001).reshape(-1,1)\n",
    "\n",
    "f_GT = my_gpGT.posterior_mean(x_pred.reshape(-1,1))[\"f(x)\"]\n",
    "v_GT = my_gpGT.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(x_pred[:,0],f_GT, color = \"red\", linewidth = 3,label='Fitted Model')\n",
    "plt.fill_between(x_pred[:,0],f_GT - 2. * np.sqrt(v_GT), f_GT + 2. * np.sqrt(v_GT), alpha = 0.5, color = \"grey\", label='Confidence Interval')\n",
    "plt.scatter(x_data_all,y_data_all, color = \"blue\",s = 250, label='Data Points') # Training Data\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size) \n",
    "plt.ylabel(\"Energy\",fontsize=label_size)\n",
    "plt.ylim(150,370)\n",
    "plt.yticks([150, 200, 250, 300, 350])\n",
    "plt.xlim(0,600)\n",
    "plt.xticks([0,150,300,450,600])\n",
    "plt.legend(fontsize=label_size,frameon=False,loc='lower left')\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\\plt.savefig('/results/Dataset 1 Figures/With Stopping/POF_my_plot_GT.png') # saving plot with a unique name \n",
    "plt.savefig('/results/Dataset 1 Figures/With Stopping/Ground Truth Fit.png') # saving plot with a unique name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 percent failed after:  331.2\n"
     ]
    }
   ],
   "source": [
    "d_GT = make_distribution(f_GT,v_GT,x_pred)[:,800].T\n",
    "\n",
    "d_GT = d_GT/(np.sum(d_GT) * dx)\n",
    "\n",
    "\n",
    "# Number cycles where 5% of batteries failed \n",
    "cum_distr = np.cumsum(d_GT)*dx\n",
    "index_95failed = np.argmin(abs(cum_distr - 0.05))\n",
    "print(\"95 percent failed after: \", x_pred[index_95failed,0])\n",
    "  \n",
    "epsilon_for_failure_distribution = 1e-10\n",
    "d_GT = np.maximum(d_GT, epsilon_for_failure_distribution)\n",
    "\n",
    "x_pred1 = np.append(x_pred,np.array([600.1,700]))\n",
    "d_GT1 = np.append(d_GT,np.array([1e-10,1e-10]))\n",
    "\n",
    "#print(d.shape,x_pred[:,0].shape)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred1, d_GT1,linewidth = 5)\n",
    "plt.xlabel(\"cycle number\",fontsize=label_size)\n",
    "plt.ylabel(\" \",fontsize=label_size)\n",
    "plt.axvline(x=x_pred[index_95failed,0], color='red', linestyle='--',linewidth = 5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.fill_between(x_pred1, d_GT1, where=(x_pred1 <= x_pred[index_95failed, 0]),interpolate=True, color='red', alpha=0.3)\n",
    "plt.yticks([])\n",
    "plt.xlim([200,700])\n",
    "plt.savefig('/results/Dataset 1 Figures/With Stopping/POF_my_plot_GT.png') # saving plot with a unique name \n",
    "#plt.close() # closing the plot to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __________________________________________________________________\n",
    "# Sequence of Experiments \n",
    "# Multiple Trials, Different batteries sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the difference between the thr All-Data GP and a one-Battery GP\n",
    "\n",
    "def current_battery_GP_calculations(cycle_number, energy_data, test_battery, battery, \n",
    "                                    my_trained_hps, kernel, mean2, my_noise, initial_capacity, x_data_old ,y_data_old,\n",
    "                                    my_calc_kl,error_threshold , current_battery):\n",
    "\n",
    "    my_battery_stopping_condition = False\n",
    "\n",
    "    x_data_current_battery = cycle_number[0:test_battery+1]\n",
    "    y_data_current_battery = energy_data[battery,0:test_battery+1]\n",
    "    \n",
    "    my_gp_current_battery = GPOptimizer(x_data_current_battery,y_data_current_battery,\n",
    "                         init_hyperparameters = my_trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "                         #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "                         compute_device='cpu', \n",
    "                         gp_kernel_function=kernel_nn, \n",
    "                         gp_kernel_function_grad=None, \n",
    "                         gp_mean_function=mean2, \n",
    "                         gp_mean_function_grad=None,\n",
    "                         gp_noise_function=my_noise,\n",
    "                         normalize_y=False,\n",
    "                         sparse_mode=False,\n",
    "                         gp2Scale = False,\n",
    "                         store_inv=False, \n",
    "                         ram_economy=False, \n",
    "                         args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n",
    "    \n",
    "    \n",
    "    my_gp_Old_with_current_battery = GPOptimizer(x_data_old,y_data_old,\n",
    "                         init_hyperparameters = my_trained_hps,  # we need enough of those for kernel, noise and prior mean functions\n",
    "                         #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "                         compute_device='cpu', \n",
    "                         gp_kernel_function=kernel_nn, \n",
    "                         gp_kernel_function_grad=None, \n",
    "                         gp_mean_function=mean2, \n",
    "                         gp_mean_function_grad=None,\n",
    "                         gp_noise_function=my_noise,\n",
    "                         normalize_y=False,\n",
    "                         sparse_mode=False,\n",
    "                         gp2Scale = False,\n",
    "                         store_inv=False, \n",
    "                         ram_economy=False, \n",
    "                         args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n",
    "        \n",
    "\n",
    "    \n",
    "    ####################################################\n",
    "    x_new_lower_limit = cycle_number[test_battery]\n",
    "    \n",
    "    x_pred2 = np.linspace(x_new_lower_limit,600,50).reshape(-1,1)\n",
    "    \n",
    "    my_posterior_mean_CurrentBattery = my_gp_current_battery.posterior_mean(x_pred2)[\"f(x)\"]\n",
    "    my_posterior_covariance_CurrentBattery = my_gp_current_battery.posterior_covariance(x_pred2)['S'] + 1e-7 * np.eye(len(x_pred2)) \n",
    "    \n",
    "    my_posterior_mean_AllData = my_gp_Old_with_current_battery.posterior_mean(x_pred2)[\"f(x)\"]\n",
    "    my_posterior_covariance_AllData = my_gp_Old_with_current_battery.posterior_covariance(x_pred2)['S'] + 1e-7 * np.eye(len(x_pred2))\n",
    "    \n",
    "    diff_CurrentBattery_AllData = my_gp_Old_with_current_battery.kl_div(my_posterior_mean_CurrentBattery,my_posterior_mean_AllData,my_posterior_covariance_CurrentBattery,my_posterior_covariance_AllData)\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    my_calc_kl = np.append(my_calc_kl,diff_CurrentBattery_AllData)\n",
    "    \n",
    "    # Calculate the cumulative sum up to the current index\n",
    "    cumulative_sum = np.sum(my_calc_kl)\n",
    "    my_thresholded_average = error_threshold * cumulative_sum / len(my_calc_kl)\n",
    "        \n",
    "    \n",
    "    if diff_CurrentBattery_AllData < my_thresholded_average and test_battery > 2:\n",
    "        print(\"diff_CurrentBattery_AllData: \", diff_CurrentBattery_AllData)\n",
    "        print(\"my_thresholded_average: \", my_thresholded_average)\n",
    "        print(\"Battery \", current_battery, \"  is not informative, it is SKIPPED\")\n",
    "        my_battery_stopping_condition = True\n",
    "    \n",
    "    \n",
    "    return np.array([diff_CurrentBattery_AllData, my_thresholded_average,my_calc_kl,my_battery_stopping_condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results/Dataset 1 Figures/With Stopping/Trial_6\n",
      "My Counter:  0\n",
      "A:  13 0\n",
      "B:  10 0\n",
      "C:  12 0\n",
      "D:  16 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/329689771.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/329689771.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "My Counter:  2\n",
      "A:  13 2\n",
      "B:  10 2\n",
      "C:  12 2\n",
      "D:  16 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/329689771.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/329689771.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "My Counter:  4\n",
      "A:  13 4\n",
      "B:  10 4\n",
      "C:  12 4\n",
      "D:  16 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3366/329689771.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 159\u001b[0m\n\u001b[1;32m    141\u001b[0m my_gp_All \u001b[38;5;241m=\u001b[39m GPOptimizer(x_data,y_data,\n\u001b[1;32m    142\u001b[0m                              init_hyperparameters \u001b[38;5;241m=\u001b[39m init_hyperparameters,  \u001b[38;5;66;03m# we need enough of those for kernel, noise and prior mean functions\u001b[39;00m\n\u001b[1;32m    143\u001b[0m                              \u001b[38;5;66;03m#noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m                              ram_economy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    155\u001b[0m                              args\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_for_GP_training:\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mmy_gp_All\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglobal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m my_trained_hps \u001b[38;5;241m=\u001b[39m my_gp_All\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Saving Trained Hyperparameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/gpcam/gp_optimizer.py:526\u001b[0m, in \u001b[0;36mGPOptimizer.train\u001b[0;34m(self, objective_function, objective_function_gradient, objective_function_hessian, hyperparameter_bounds, init_hyperparameters, method, pop_size, tolerance, max_iter, local_optimizer, global_optimizer, constraints, dask_client)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m         objective_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m         constraints\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    457\u001b[0m         dask_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    This function finds the maximum of the log marginal likelihood and therefore trains the GP (synchronously).\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m    This can be done on a remote cluster/computer by specifying the method to be 'hgdl' and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m        Returned are the hyperparameters, however, the GP is automatically updated.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_hyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdask_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdask_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:634\u001b[0m, in \u001b[0;36mGP.train\u001b[0;34m(self, objective_function, objective_function_gradient, objective_function_hessian, hyperparameter_bounds, init_hyperparameters, method, pop_size, tolerance, max_iter, local_optimizer, global_optimizer, constraints, dask_client)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m objective_function_gradient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: objective_function_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_log_likelihood_gradient\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m objective_function_hessian \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: objective_function_hessian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_log_likelihood_hessian\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_client\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKV, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVinvY, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVlogdet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactorization_obj, \\\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVinv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_mean_vec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_GPpriorV(\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters, calc_inv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_inv)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:935\u001b[0m, in \u001b[0;36mGP._optimize_log_likelihood\u001b[0;34m(self, objective_function, objective_function_gradient, objective_function_hessian, starting_hps, hp_bounds, method, max_iter, pop_size, tolerance, constraints, local_optimizer, global_optimizer, dask_client)\u001b[0m\n\u001b[1;32m    933\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtermination tolerance: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, tolerance)\n\u001b[1;32m    934\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounds: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, hp_bounds)\n\u001b[0;32m--> 935\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhp_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_hps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    948\u001b[0m Eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_log_likelihood(hyperparameters)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/optimize/_differentialevolution.py:402\u001b[0m, in \u001b[0;36mdifferential_evolution\u001b[0;34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, seed, callback, disp, polish, init, atol, updating, workers, constraints, x0, integrality, vectorized)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# using a context manager means that any created Pool objects are\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# cleared up.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DifferentialEvolutionSolver(func, bounds, args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    388\u001b[0m                                  strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m    389\u001b[0m                                  maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                  integrality\u001b[38;5;241m=\u001b[39mintegrality,\n\u001b[1;32m    401\u001b[0m                                  vectorized\u001b[38;5;241m=\u001b[39mvectorized) \u001b[38;5;28;01mas\u001b[39;00m solver:\n\u001b[0;32m--> 402\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/optimize/_differentialevolution.py:1022\u001b[0m, in \u001b[0;36mDifferentialEvolutionSolver.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxiter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;66;03m# evolve the population by a generation\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m         warning_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/optimize/_differentialevolution.py:1409\u001b[0m, in \u001b[0;36mDifferentialEvolutionSolver.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     feasible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     cv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d([\u001b[38;5;241m0.\u001b[39m])\n\u001b[0;32m-> 1409\u001b[0m     energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# compare trial and population member\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/_lib/_util.py:360\u001b[0m, in \u001b[0;36m_FunctionWrapper.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:1058\u001b[0m, in \u001b[0;36mGP.neg_log_likelihood\u001b[0;34m(self, hyperparameters)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneg_log_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, hyperparameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m    Function that computes the marginal log-likelihood\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    negative log marginal likelihood of the data : float\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:1039\u001b[0m, in \u001b[0;36mGP.log_likelihood\u001b[0;34m(self, hyperparameters)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     K, KV, KVinvY, KVlogdet, FO, KVinv, mean, cov \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKV, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVinvY, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVlogdet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactorization_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKVinv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_mean_vec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     K, KV, KVinvY, KVlogdet, FO, KVinv, mean, cov \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 1039\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_GPpriorV\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_inv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m KVinvY)) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m KVlogdet) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m n \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:1201\u001b[0m, in \u001b[0;36mGP._compute_GPpriorV\u001b[0;34m(self, x_data, y_data, hyperparameters, calc_inv)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransferring the covariance matrix to host done after \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st,\n\u001b[1;32m   1199\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m seconds. sparsity = \u001b[39m\u001b[38;5;124m\"\u001b[39m, Ksparsity, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_K\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;66;03m# check if shapes are correct\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m K\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m V\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoise covariance and prior covariance not of the same shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/fvgp/gp.py:1276\u001b[0m, in \u001b[0;36mGP._compute_K\u001b[0;34m(self, hyperparameters)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_K\u001b[39m(\u001b[38;5;28mself\u001b[39m, hyperparameters):\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"computes the covariance matrix from the kernel\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1276\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m, in \u001b[0;36mkernel_nn\u001b[0;34m(x1, x2, hps, obj)\u001b[0m\n\u001b[1;32m     40\u001b[0m d \u001b[38;5;241m=\u001b[39m get_distance_matrix(x1_nn,x2_nn)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Main Function\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mhps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtotal_num_of_NN_hps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquared_exponential_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#100\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m k\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_pred = np.linspace(0,1500,1001).reshape(-1,1)\n",
    "\n",
    "\n",
    "init_hyperparameters = my_GT_hps\n",
    "\n",
    "\n",
    "label_size = 30\n",
    "error_threshold = 0.5 #Threshold used to stop the experiments. A value of 0 means no stopping criterion is implemented\n",
    "\n",
    "\n",
    "num_versions = 30\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "# Generate 10 different random sequences\n",
    "np.random.seed(random_seed)  # Set the seed outside the loop\n",
    "random_sequences = [np.random.permutation(20) for _ in range(num_versions)]\n",
    "\n",
    "#for t in range(0,num_versions):\n",
    "for t in range(6,7):\n",
    "\n",
    "    # Specify the path for the new folder\n",
    "    new_folder_path = f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}\"\n",
    "\n",
    "    # Create the folder\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "    %cd {new_folder_path}\n",
    "    \n",
    "    # Check if the csv files exist and delete them if present\n",
    "    if os.path.exists(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Failure_Cycle_Number.csv\"): os.remove(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Failure_Cycle_Number.csv\")\n",
    "    if os.path.exists(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Trained Hyperparameters.csv\"): os.remove(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Trained Hyperparameters.csv\")    \n",
    "    if os.path.exists(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Total Correlation.csv\"): os.remove(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Total Correlation.csv\")\n",
    "        \n",
    "        \n",
    "    # battery Sequence\n",
    "    my_sequence_of_batteries = random_sequences[t]\n",
    "    counter_for_sequence_of_batteries = 4\n",
    "\n",
    "    battery_A = my_sequence_of_batteries[0]\n",
    "    battery_B = my_sequence_of_batteries[1]\n",
    "    battery_C = my_sequence_of_batteries[2]\n",
    "    battery_D = my_sequence_of_batteries[3]\n",
    "\n",
    "    # Initializing the battery tests\n",
    "    test_battery_A = 0\n",
    "    test_battery_B = 0\n",
    "    test_battery_C = 0\n",
    "    test_battery_D = 0\n",
    "\n",
    "    stop_all_battery_A = False\n",
    "    stop_all_battery_B = False\n",
    "    stop_all_battery_C = False\n",
    "    stop_all_battery_D = False\n",
    "\n",
    "    # Do I retrain the GP Model everytime I add a point\n",
    "    condition_for_GP_training = True\n",
    "    my_calc_kl = np.empty((0, 1))\n",
    "\n",
    "    x_data = np.empty((0, 1))\n",
    "    y_data = np.empty((0, 1))\n",
    "\n",
    "    my_counter = 0\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if my_counter% 2 == 0: \n",
    "            print(\"My Counter: \", my_counter)\n",
    "            print(\"A: \", battery_A,  test_battery_A)\n",
    "            print(\"B: \", battery_B,  test_battery_B)\n",
    "            print(\"C: \", battery_C,  test_battery_C)\n",
    "            print(\"D: \", battery_D,  test_battery_D)\n",
    "\n",
    "\n",
    "        stop_this_battery_A = False\n",
    "        stop_this_battery_B = False\n",
    "        stop_this_battery_C = False\n",
    "        stop_this_battery_D = False\n",
    "\n",
    "        # Adding the required experimental data\n",
    "\n",
    "        # If there is still data to be added\n",
    "        if not(stop_all_battery_A):\n",
    "            battery_A_x_data = cycle_number[test_battery_A:test_battery_A+1]\n",
    "            battery_A_y_data = energy_data[battery_A,test_battery_A:test_battery_A+1] \n",
    "        else: # Otherwise\n",
    "            battery_A_x_data = np.empty((0))\n",
    "            battery_A_y_data = np.empty((0))    \n",
    "\n",
    "        # If there is still data to be added\n",
    "        if not(stop_all_battery_B):\n",
    "            battery_B_x_data = cycle_number[test_battery_B:test_battery_B+1]\n",
    "            battery_B_y_data = energy_data[battery_B,test_battery_B:test_battery_B+1]\n",
    "        else: # Otherwise\n",
    "            battery_B_x_data = np.empty((0))\n",
    "            battery_B_y_data = np.empty((0))\n",
    "\n",
    "\n",
    "        # If there is still data to be added\n",
    "        if not(stop_all_battery_C):\n",
    "            battery_C_x_data = cycle_number[test_battery_C:test_battery_C+1]\n",
    "            battery_C_y_data = energy_data[battery_C,test_battery_C:test_battery_C+1]\n",
    "        else: # Otherwise\n",
    "            battery_C_x_data = np.empty((0))\n",
    "            battery_C_y_data = np.empty((0))\n",
    "\n",
    "        # If there is still data to be added\n",
    "        if not(stop_all_battery_D):\n",
    "            battery_D_x_data = cycle_number[test_battery_D:test_battery_D+1]\n",
    "            battery_D_y_data = energy_data[battery_D,test_battery_D:test_battery_D+1]\n",
    "        else: # Otherwise\n",
    "            battery_D_x_data = np.empty((0))\n",
    "            battery_D_y_data = np.empty((0))\n",
    "\n",
    "        x_to_append = np.concatenate((battery_A_x_data,battery_B_x_data,battery_C_x_data,battery_D_x_data))\n",
    "        y_to_append = np.concatenate((battery_A_y_data,battery_B_y_data,battery_C_y_data,battery_D_y_data))    \n",
    "\n",
    "        # Break the while loop when there are no more tests to be done\n",
    "        if np.size(x_to_append) == 0:\n",
    "            break\n",
    "\n",
    "        # To be used for the GP of All Data when comparing the expected information gain,\n",
    "        # without the batteries being tested now, except for the one we are interested in\n",
    "        x_data_old = np.array(x_data)\n",
    "        y_data_old = np.array(y_data)\n",
    "\n",
    "        x_data = np.append(x_data, x_to_append)    \n",
    "        y_data = np.append(y_data, y_to_append)   \n",
    "\n",
    "\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "        # Main GP With All Data\n",
    "\n",
    "        # Finding the mean of the initial capacity to be entered to the code\n",
    "        my_ind = np.where(x_data<=10)\n",
    "        initial_capacity = np.mean(y_data[my_ind[0]])\n",
    "\n",
    "        my_gp_All = GPOptimizer(x_data,y_data,\n",
    "                                     init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "                                     #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "                                     compute_device='cpu', \n",
    "                                     gp_kernel_function=kernel_nn, \n",
    "                                     gp_kernel_function_grad=None, \n",
    "                                     gp_mean_function=mean2, \n",
    "                                     gp_mean_function_grad=None,\n",
    "                                     gp_noise_function=my_noise,\n",
    "                                     normalize_y=False,\n",
    "                                     sparse_mode=False,\n",
    "                                     gp2Scale = False,\n",
    "                                     store_inv=False, \n",
    "                                     ram_economy=False, \n",
    "                                     args=np.array([nodes_num,total_num_of_NN_hps,initial_capacity,my_trained_NN_hps]))\n",
    "\n",
    "\n",
    "        if condition_for_GP_training:\n",
    "            my_gp_All.train(hyperparameter_bounds=bounds, method='global')\n",
    "\n",
    "        my_trained_hps = my_gp_All.hyperparameters\n",
    "\n",
    "        # Saving Trained Hyperparameters\n",
    "        row_hyperparameters = [my_counter] +  list(my_trained_hps)\n",
    "\n",
    "        # Write the row_entropies to the CSV file\n",
    "        csv_file = open(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Trained Hyperparameters.csv\", 'a', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(row_hyperparameters)\n",
    "        csv_file.close() # Close the CSV file\n",
    "\n",
    "\n",
    "        f = my_gp_All.posterior_mean(x_pred)[\"f(x)\"]\n",
    "        v =  my_gp_All.posterior_covariance(x_pred, variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "        vnn =  my_gp_All.posterior_covariance(x_pred, variance_only=False, add_noise=False)[\"v(x)\"]      \n",
    "\n",
    "         # Plotting the data\n",
    "        plt.figure(figsize = (10,10))\n",
    "        plt.plot(x_pred[:,0],f, color = \"red\", linewidth = 5,label=' ')\n",
    "        plt.fill_between(x_pred[:,0],f - 2. * np.sqrt(v), f + 2. * np.sqrt(v), alpha = 0.4, color = \"grey\", label=' ')\n",
    "        plt.scatter(x_data,y_data, s=250, color = \"blue\",label=' ') # Training Data\n",
    "        #plt.xlabel(\"Cycle Number\",fontsize=label_size) \n",
    "        #plt.ylabel(\"Energy\",fontsize=label_size)\n",
    "        plt.xlabel(\" \",fontsize=label_size) \n",
    "        plt.ylabel(\" \",fontsize=label_size)\n",
    "        plt.ylim(150,370)\n",
    "        #plt.yticks([150, 200, 250, 300, 350])\n",
    "        plt.yticks([])\n",
    "        plt.xlim(0,600)\n",
    "        #plt.xticks([0,150,300,450,600])\n",
    "        plt.xticks([])\n",
    "        plt.legend(fontsize=label_size,frameon=False,loc='lower left')\n",
    "        plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "        plt.savefig(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/my_plot_{my_counter}.pdf\", bbox_inches='tight')\n",
    "        plt.close() \n",
    "       \n",
    "\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "\n",
    "        # Calculate Probability of Failure Distribution and finding the cycle at which 5% failed <90% initial energy \n",
    "\n",
    "        d = make_distribution(f,v,x_pred)[:,800].T\n",
    "        d = d/(np.sum(d) * dx)\n",
    "\n",
    "        # Number cycles where 5% of batteries failed \n",
    "        cum_distr = np.cumsum(d)*dx\n",
    "        index_95failed = np.argmin(abs(cum_distr - 0.05))\n",
    "\n",
    "        d = np.maximum(d, epsilon_for_failure_distribution)\n",
    "\n",
    "        # Calculating the KL divergence between the current probability of failure and the GT probability of failrue\n",
    "        my_kl_between_pof = entropy(d, d_GT)\n",
    "\n",
    "        row_failures = [my_counter, x_pred[index_95failed,0], my_kl_between_pof]\n",
    "        csv_file = open(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Failure_Cycle_Number.csv\", 'a', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(row_failures)\n",
    "        csv_file.close() # Close the CSV file\n",
    "\n",
    "\n",
    "        if my_kl_between_pof<=0.05:\n",
    "            print(\"No Training\")\n",
    "            condition_for_GP_training = False\n",
    "\n",
    "        my_counter +=1\n",
    "\n",
    "\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "\n",
    "        # Stop the battery that is not informative\n",
    "\n",
    "        if my_counter > 50 : # Start checking after the first set of 4 batteries are done\n",
    "            diff_CurrentBattery_AllData_A = 0\n",
    "            diff_CurrentBattery_AllData_B = 0\n",
    "            diff_CurrentBattery_AllData_C = 0\n",
    "            diff_CurrentBattery_AllData_D = 0\n",
    "\n",
    "            # Battery A\n",
    "            if not(stop_all_battery_A):\n",
    "                [diff_CurrentBattery_AllData_A, my_thresholded_average_A,my_calc_kl,stop_this_battery_A] = current_battery_GP_calculations(cycle_number, energy_data, test_battery_A, battery_A, \n",
    "                                                                                                 my_trained_hps, kernel_nn, mean2, my_noise, initial_capacity, x_data_old ,y_data_old,\n",
    "                                                                                                 my_calc_kl,error_threshold,\"A\") \n",
    "            # Battery B\n",
    "            if not(stop_all_battery_B):\n",
    "                [diff_CurrentBattery_AllData_B, my_thresholded_average_B,my_calc_kl,stop_this_battery_B] = current_battery_GP_calculations(cycle_number, energy_data, test_battery_B, battery_B, \n",
    "                                                                                                 my_trained_hps, kernel_nn, mean2, my_noise, initial_capacity, x_data_old ,y_data_old,\n",
    "                                                                                                  my_calc_kl,error_threshold,\"B\")\n",
    "            # Battery C   \n",
    "            if not(stop_all_battery_C):\n",
    "                [diff_CurrentBattery_AllData_C, my_thresholded_average_C,my_calc_kl,stop_this_battery_C] = current_battery_GP_calculations(cycle_number, energy_data, test_battery_C, battery_C, \n",
    "                                                                                                 my_trained_hps, kernel_nn, mean2, my_noise, initial_capacity, x_data_old ,y_data_old,\n",
    "                                                                                                  my_calc_kl,error_threshold,\"C\")\n",
    "            # Battery D  \n",
    "            if not(stop_all_battery_D):\n",
    "                [diff_CurrentBattery_AllData_D, my_thresholded_average_D,my_calc_kl,stop_this_battery_D] = current_battery_GP_calculations(cycle_number, energy_data, test_battery_D, battery_D, \n",
    "                                                                                                 my_trained_hps, kernel_nn, mean2, my_noise, initial_capacity, x_data_old ,y_data_old,\n",
    "                                                                                                  my_calc_kl,error_threshold,\"D\")\n",
    "\n",
    "\n",
    "\n",
    "            # Create a row with the extracted values\n",
    "            row_entropies = [my_counter, diff_CurrentBattery_AllData_A, diff_CurrentBattery_AllData_B, \n",
    "                         diff_CurrentBattery_AllData_C, diff_CurrentBattery_AllData_D]\n",
    "\n",
    "            # Write the row_entropies to the CSV file\n",
    "            csv_file = open(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Total Correlation.csv\", 'a', newline='')\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(row_entropies)\n",
    "            csv_file.close() # Close the CSV file\n",
    "\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "        #########################################################################\n",
    "\n",
    "\n",
    "        # Update for Battery A\n",
    "        test_battery_A = test_battery_A + 1\n",
    "\n",
    "        # If current battery A finished testing or If Battery A is stopped      \n",
    "        if test_battery_A > 49 or stop_this_battery_A:     \n",
    "            if counter_for_sequence_of_batteries<20:\n",
    "                battery_A = my_sequence_of_batteries[counter_for_sequence_of_batteries]\n",
    "                counter_for_sequence_of_batteries +=1\n",
    "                test_battery_A = 0\n",
    "            else:\n",
    "                stop_all_battery_A = True\n",
    "\n",
    "        #############################\n",
    "        # Update for Battery B\n",
    "        test_battery_B = test_battery_B + 1\n",
    "\n",
    "        # If current battery B finished testing or If Battery B is stopped      \n",
    "        if test_battery_B > 49 or stop_this_battery_B:\n",
    "            if counter_for_sequence_of_batteries<20:\n",
    "                battery_B = my_sequence_of_batteries[counter_for_sequence_of_batteries]\n",
    "                counter_for_sequence_of_batteries +=1\n",
    "                test_battery_B = 0\n",
    "            else:\n",
    "                stop_all_battery_B = True\n",
    "\n",
    "        #############################   \n",
    "        # Update for Battery C\n",
    "        test_battery_C = test_battery_C + 1\n",
    "\n",
    "        # If current battery C finished testing or If Battery A is stopped      \n",
    "        if test_battery_C > 49 or stop_this_battery_C:\n",
    "            if counter_for_sequence_of_batteries<20:\n",
    "                battery_C = my_sequence_of_batteries[counter_for_sequence_of_batteries]\n",
    "                counter_for_sequence_of_batteries +=1\n",
    "                test_battery_C = 0\n",
    "            else:\n",
    "                stop_all_battery_C = True\n",
    "\n",
    "        ############################# \n",
    "        # Update for Battery D\n",
    "        test_battery_D = test_battery_D + 1\n",
    "\n",
    "        # If current battery D finished testing or If Battery D is stopped      \n",
    "        if test_battery_D > 49 or stop_this_battery_D:\n",
    "            if counter_for_sequence_of_batteries<20:\n",
    "                battery_D = my_sequence_of_batteries[counter_for_sequence_of_batteries]\n",
    "                counter_for_sequence_of_batteries +=1\n",
    "                test_battery_D = 0\n",
    "            else:\n",
    "                stop_all_battery_D = True       \n",
    "\n",
    "        #############################   \n",
    "\n",
    "\n",
    "        print(\"===========================\")\n",
    "\n",
    "\n",
    "    print(\"Training Complete for All Batteries\")\n",
    "\n",
    "    # Save to .npy file\n",
    "    np.save(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Training_X_data.npy\", x_data)\n",
    "    np.save(f\"/results/Dataset 1 Figures/With Stopping/Trial_{t}/Training_Y_data.npy\", y_data)\n",
    "    \n",
    "    \n",
    "    %cd ..\n",
    "    \n",
    "    row_num_experiments = [t, x_data.shape[0]]\n",
    "    # Write the number of experiments done in this trial to the CSV file\n",
    "    csv_file = open(f\"/results/Dataset 1 Figures/With Stopping/Number of Experiments.csv\", 'a', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(row_num_experiments)\n",
    "    csv_file.close() # Close the CSV file\n",
    "\n",
    "print(\"All Trials are done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO63OquyMThIosCiq1LW7Xx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
