{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93da119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpcam import GPOptimizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc540f0b",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d106b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5546, 9477, 2231, 4437, 7059, 5259, 8330, 1068, 8214, 5888, 3275, 6845, 7671, 299, 5038, 3503, 8673, 2236, 3644, 4980, 993, 7545, 654, 1418, 6090, 7936, 8792, 6910, 2933, 2382, 9730, 8476, 1882, 7986, 7091, 4813, 3086, 3908, 1539, 8567, 2152, 5738, 8646, 9692, 2661, 6766, 7230, 512, 758, 2881]\n",
      "max y:  529.5870098158235\n",
      "x data:  (2500, 1)\n",
      "y data:  (2500, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3b7b4c7e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_data = np.load(\"/data/Synthetic Data Generation_1/my_synthetic_energy.npy\")\n",
    "cycle_number = np.load(\"/data/Synthetic Data Generation_1/my_synthetic_cycleNum.npy\")\n",
    "\n",
    "\n",
    "label_size = 30\n",
    "\n",
    "num_of_datasets = 50\n",
    "\n",
    "considered_batteries = ([5546, 9477, 2231, 4437, 7059, 5259, 8330, 1068, 8214, 5888, 3275, 6845, 7671, \n",
    "                         299, 5038, 3503, 8673, 2236, 3644, 4980, 993, 7545, 654, 1418, 6090, 7936, 8792, \n",
    "                         6910, 2933, 2382, 9730, 8476, 1882, 7986, 7091, 4813, 3086, 3908, 1539, 8567, 2152, \n",
    "                         5738, 8646, 9692, 2661, 6766, 7230, 512, 758, 2881])\n",
    "\n",
    "\n",
    "print(considered_batteries)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "for i in considered_batteries: plt.scatter(cycle_number,energy_data[int(i)])\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.xlabel(\"Cycle Number\",fontsize=label_size)\n",
    "plt.ylabel(\"Quantity of Interest\",fontsize=label_size)\n",
    "plt.show()\n",
    "\n",
    "print(\"max y: \", np.max(energy_data))\n",
    "\n",
    "# Initializing the data to fit the GP model\n",
    "data_size = num_of_datasets\n",
    "\n",
    "# All Data\n",
    "x_data = np.tile(cycle_number, data_size).reshape(-1, 1) # repeat cycle 20 times to create x_data\n",
    "y_data = np.vstack(energy_data[considered_batteries, :].T).reshape(-1, 1)\n",
    "x_pred = np.linspace(0,1000,1001).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"x data: \", x_data.shape)\n",
    "print(\"y data: \", y_data.shape)\n",
    "        \n",
    "    \n",
    "plt.scatter(x_data,y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b7ee8-e9a4-4ca6-9529-47fb5294b5a5",
   "metadata": {},
   "source": [
    "# Creating the Subfolder in Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc8787c1-fa42-4f79-94ca-f016d73f3508",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m new_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results/Methods Figures\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the folder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(new_folder_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path for the new folder\n",
    "new_folder_path = f\"/results/Methods Figures\"\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(new_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d154103",
   "metadata": {},
   "source": [
    "# Deep Kernel Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7627ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# All remaining code asssumes that the NN architecture is made up of two hidden layers and same number of nodes\n",
    "# If other architectures are used, the indices of the hyperparameters and their boudsn need to be changed accordingly\n",
    "# Number of nodes can be varied as the user prefer\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nodes_num = 5\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.layer1 = nn.Linear(1, self.nodes_num)\n",
    "        self.layer2 = nn.Linear(self.nodes_num, self.nodes_num)\n",
    "        self.layer3 = nn.Linear(self.nodes_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        #print(x)\n",
    "        x = torch.Tensor(x)\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        x = torch.nn.functional.relu(self.layer3(x))\n",
    "        return x.detach().numpy()\n",
    "\n",
    "    def set_weights(self,w1,w2,w3):\n",
    "      with torch.no_grad():\n",
    "        self.layer1.weight = nn.Parameter(torch.from_numpy(w1).float())\n",
    "        self.layer2.weight = nn.Parameter(torch.from_numpy(w2).float())\n",
    "        self.layer3.weight = nn.Parameter(torch.from_numpy(w3).float())\n",
    "\n",
    "    def set_biases(self,b1,b2,b3):\n",
    "      with torch.no_grad():\n",
    "        self.layer1.bias = nn.Parameter(torch.from_numpy(b1).float())\n",
    "        self.layer2.bias = nn.Parameter(torch.from_numpy(b2).float())\n",
    "        self.layer3.bias = nn.Parameter(torch.from_numpy(b3).float())\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.layer1.weight, self.layer2.weight, self.layer3.weight\n",
    "    def get_biases(self):\n",
    "        return self.layer1.bias, self.layer2.bias, self.layer3.bias\n",
    "\n",
    "n = Network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8677fad",
   "metadata": {},
   "source": [
    "# Defining GP Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04cfe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the squared Exponential Function\n",
    "def get_distance_matrix(x1,x2):\n",
    "    d = np.zeros((len(x1),len(x2)))\n",
    "    for i in range(x1.shape[1]):\n",
    "        d += (x1[:,i].reshape(-1, 1) - x2[:,i])**2\n",
    "    return np.sqrt(d)\n",
    "\n",
    "# For the Noise\n",
    "def s(x, my_slope, my_pow, my_intercept):\n",
    "    o = my_slope * x**my_pow + my_intercept\n",
    "    return o\n",
    "\n",
    "'''\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    my_slope     = hps[total_num_of_NN_hps+1]\n",
    "    my_pow       = hps[total_num_of_NN_hps+2]\n",
    "    my_intercept = hps[total_num_of_NN_hps+3]\n",
    "\n",
    "    noise = np.identity(len(x)) * np.outer(s(x,my_slope,my_pow,my_intercept),s(x,my_slope,my_pow,my_intercept))\n",
    "\n",
    "    return noise\n",
    "\n",
    "\n",
    "# Kernel Function\n",
    "def kernel_nn(x1,x2,hps,obj):\n",
    "\n",
    "    nodes_num           = obj.args[0]\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    \n",
    "   \n",
    "    # NN\n",
    "    n.set_weights(hps[0:nodes_num].reshape(nodes_num,1),\n",
    "                  hps[nodes_num:nodes_num**2+nodes_num].reshape(nodes_num,nodes_num),\n",
    "                  hps[nodes_num**2+nodes_num:nodes_num**2+2*nodes_num].reshape(1,nodes_num))\n",
    "\n",
    "    n.set_biases(hps[nodes_num**2+2*nodes_num:nodes_num**2+3*nodes_num].reshape(nodes_num),\n",
    "                 hps[nodes_num**2+3*nodes_num:nodes_num**2+4*nodes_num].reshape(nodes_num),\n",
    "                 np.array([hps[nodes_num**2+4*nodes_num]]))\n",
    "\n",
    "    x1_nn = n.forward(x1).reshape(-1,1)\n",
    "    x2_nn = n.forward(x2).reshape(-1,1)\n",
    "    d = get_distance_matrix(x1_nn,x2_nn)\n",
    "\n",
    "    # Main Function\n",
    "    k = hps[total_num_of_NN_hps] * obj.squared_exponential_kernel(d,200)\n",
    "    return k\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    x0 = hps[total_num_of_NN_hps+4]\n",
    "    \n",
    "    m1 = hps[total_num_of_NN_hps+5]\n",
    "    m2 = hps[total_num_of_NN_hps+6]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y\n",
    "\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    my_slope     = hps[total_num_of_NN_hps+1]\n",
    "    my_pow       = hps[total_num_of_NN_hps+2]\n",
    "    my_intercept = hps[total_num_of_NN_hps+3]\n",
    "\n",
    "    noise = np.identity(len(x)) * np.outer(s(x,my_slope,my_pow,my_intercept),s(x,my_slope,my_pow,my_intercept))\n",
    "\n",
    "    return noise\n",
    "'''\n",
    "\n",
    "def my_noise(x,hps,obj):\n",
    "\n",
    "    my_slope     = hps[1]\n",
    "    my_pow       = hps[2]\n",
    "    my_intercept = hps[3]\n",
    "\n",
    "    noise = np.identity(len(x)) * np.outer(s(x,my_slope,my_pow,my_intercept),s(x,my_slope,my_pow,my_intercept))\n",
    "\n",
    "    return noise\n",
    "\n",
    "# Kernel Function\n",
    "def kernel_nn(x1,x2,hps,obj):\n",
    "\n",
    "    nodes_num           = obj.args[0]\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    \n",
    "    trained_NN_hps = np.array([-0.349,  0.461, -0.678,  0.604,  0.747,  0.827,  0.282,  0.126,  0.572, -0.313,\n",
    "                   -0.463, -0.174, -0.619,  0.701, -0.109,  0.048, -0.318, -0.367, -0.499, -0.321,\n",
    "                    0.083,  0.668, -0.209,  0.552, -0.355,  0.192,  0.772, -0.739,  0.959, -0.772,\n",
    "                   -0.713, -0.817, -0.014,  0.077,  0.381, -196.313,  126.321,   45.903, -216.376,  \n",
    "                    126.7, -86.184, -60.536, -108.548, 67.332,  162.204,  -36.093])\n",
    "    \n",
    "    # NN\n",
    "    n.set_weights(trained_NN_hps[0:nodes_num].reshape(nodes_num,1),\n",
    "                  trained_NN_hps[nodes_num:nodes_num**2+nodes_num].reshape(nodes_num,nodes_num),\n",
    "                  trained_NN_hps[nodes_num**2+nodes_num:nodes_num**2+2*nodes_num].reshape(1,nodes_num))\n",
    "\n",
    "    n.set_biases(trained_NN_hps[nodes_num**2+2*nodes_num:nodes_num**2+3*nodes_num].reshape(nodes_num),\n",
    "                 trained_NN_hps[nodes_num**2+3*nodes_num:nodes_num**2+4*nodes_num].reshape(nodes_num),\n",
    "                 np.array([trained_NN_hps[nodes_num**2+4*nodes_num]]))\n",
    "\n",
    "    x1_nn = n.forward(x1).reshape(-1,1)\n",
    "    x2_nn = n.forward(x2).reshape(-1,1)\n",
    "    d = get_distance_matrix(x1_nn,x2_nn)\n",
    "\n",
    "    # Main Function\n",
    "    k = hps[0] * obj.squared_exponential_kernel(d,200)\n",
    "    return k\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Mean function: Two-Element piecewise function\n",
    "def mean2(x,hps,obj):\n",
    "\n",
    "    total_num_of_NN_hps = obj.args[1]\n",
    "\n",
    "    x0 = hps[4]\n",
    "    \n",
    "    m1 = hps[5]\n",
    "    m2 = hps[6]\n",
    "\n",
    "    b1  = 500\n",
    "    b2 = (m1 - m2) * x0 + b1\n",
    "\n",
    "    x = x[:,0]\n",
    "\n",
    "    y = np.where(x <= x0, m1*x + b1, m2*x + b2)\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb468ba",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49358f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data:  (2500, 1)\n",
      "y data:  (2500, 1)\n",
      "Training is Done\n"
     ]
    }
   ],
   "source": [
    "x_pred_original = np.linspace(0,1000,1001).reshape(-1,1)\n",
    "\n",
    "nodes_num = n.nodes_num\n",
    "\n",
    "total_num_of_NN_hps = nodes_num**2 + 4*nodes_num + 1  # Depends on the number of layers used\n",
    "num_of_other_hps = 7                                  # Depends on Kernel, noise and mean functions\n",
    "\n",
    "\n",
    "NN_weights_initial_hps = np.random.uniform(-1, 1,nodes_num**2+2*nodes_num)  \n",
    "NN_biases_initial_hps = np.random.uniform(-100,100,total_num_of_NN_hps-(nodes_num**2+2*nodes_num))\n",
    "\n",
    "\n",
    "other_init_hps = np.array( [ 1.04806e+02,  1.00000e-03,  1.59400e+00,  1.58100e+00,  5.08026e+02,\n",
    " -5.00000e-02, -2.53000e-01])\n",
    "   \n",
    "    \n",
    "#init_hyperparameters = np.concatenate([NN_weights_initial_hps,NN_biases_initial_hps,other_init_hps])\n",
    "\n",
    "\n",
    "init_hyperparameters = other_init_hps\n",
    "\n",
    "print(\"x data: \", x_data.shape)\n",
    "print(\"y data: \", y_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "my_gpNN = GPOptimizer(x_data,y_data,\n",
    "            #init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            init_hyperparameters = init_hyperparameters,  # we need enough of those for kernel, noise and prior mean functions\n",
    "            #noise_variances=np.ones(y_data.shape) * 0.01, #provding noise variances and a noise function will raise a warning \n",
    "            compute_device='cpu', \n",
    "            gp_kernel_function=kernel_nn, \n",
    "            gp_kernel_function_grad=None, \n",
    "            gp_mean_function=mean2, \n",
    "            gp_mean_function_grad=None,\n",
    "            gp_noise_function=my_noise,\n",
    "            normalize_y=False,\n",
    "            sparse_mode=False,\n",
    "            gp2Scale = False,\n",
    "            store_inv=False, \n",
    "            ram_economy=False, \n",
    "            args= np.array([nodes_num,total_num_of_NN_hps]))\n",
    "\n",
    "# Setting the Optimization Bounds for Hyperparameters\n",
    "bounds = np.empty((num_of_other_hps,2))\n",
    "\n",
    "# Kernel Sq Exp \n",
    "bounds[0] = np.array([100.,10000.])                             # Kernel Variance\n",
    "#bounds[total_num_of_NN_hps+1] = np.array([2.,300.])                           # Kernel Lengthscale\n",
    "\n",
    "# Noise\n",
    "bounds[1] = np.array([1e-5,1.])                            # Noise Slope\n",
    "bounds[2] = np.array([1.,5.])                            # Noise Power\n",
    "bounds[3] = np.array([0.,3.])                              # Noise Intercept\n",
    "# Mean\n",
    "bounds[4] = np.array([200.,700.])                          # Mean Piecewise Intersection point\n",
    "bounds[5] = np.array([-1e-1,-1e-3])                        # Mean Slope 1\n",
    "bounds[6] = np.array([-5e-1,-1e-3])                        # Mean Slope 2\n",
    "\n",
    "\n",
    "'''\n",
    "bounds = np.empty((total_num_of_NN_hps + num_of_other_hps,2))\n",
    "\n",
    "# NN\n",
    "bounds[0:nodes_num**2+2*nodes_num] = np.array([-1.,1.])                      # Weights NN: Define spread and shift in output\n",
    "bounds[nodes_num**2+2*nodes_num:total_num_of_NN_hps] = np.array([-100.,100.])#Biases of NN: Define shift in output\n",
    "\n",
    "# Kernel Sq Exp \n",
    "bounds[total_num_of_NN_hps] = np.array([1000.,10000.])                             # Kernel Variance\n",
    "#bounds[total_num_of_NN_hps+1] = np.array([2.,300.])                           # Kernel Lengthscale\n",
    "\n",
    "# Noise\n",
    "bounds[total_num_of_NN_hps+1] = np.array([1e-5,1.])                            # Noise Slope\n",
    "bounds[total_num_of_NN_hps+2] = np.array([1.,5.])                            # Noise Power\n",
    "bounds[total_num_of_NN_hps+3] = np.array([0.,3.])                              # Noise Intercept\n",
    "# Mean\n",
    "bounds[total_num_of_NN_hps+4] = np.array([200.,400.])                          # Mean Piecewise Intersection point\n",
    "bounds[total_num_of_NN_hps+5] = np.array([-2e-1,-1e-2])                        # Mean Slope 1\n",
    "bounds[total_num_of_NN_hps+6] = np.array([-2e-1,-1e-2])                        # Mean Slope 2\n",
    "'''\n",
    "\n",
    "\n",
    "#my_gpNN.train(hyperparameter_bounds=bounds, method = \"global\")\n",
    "\n",
    "\n",
    "print(\"Training is Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e1de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_nn = n.forward(x_pred)\n",
    "\n",
    "label_size = 30\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_pred_original,x_pred_nn,linewidth = 7,color = 'blue')\n",
    "plt.xlabel(\"Original Domain ($\\mathcal{X}$)\",fontsize=label_size)\n",
    "plt.ylabel(\"Transformed Domain ($\\mathcal{X}^*$)\",fontsize=label_size)\n",
    "#plt.title(\"Cycle Number\",fontsize=label_size)\n",
    "plt.ylim([0,100])\n",
    "plt.yticks([0,25,50,75,100])\n",
    "plt.xlim([0,1000])\n",
    "plt.xticks([0,250,500,750,1000])\n",
    "plt.tick_params(axis='both', which='major', labelsize=label_size) # Set the font size of the tick labels on the x and y axes\n",
    "plt.savefig('/results/Methods Figures/NN Transformation.pdf',  bbox_inches='tight') # saving plot with a unique name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dfcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
